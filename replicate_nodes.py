"""
Replicate ComfyUI ç¯€é»æ•´åˆ
æ”¯æ´ Lipsyncã€é€šç”¨æ¨¡å‹ã€å‹•æ…‹åƒæ•¸ç­‰åŠŸèƒ½
"""

import os
import shutil
import torch
import numpy as np
import cv2
from .replicate_api import SyncAPI, ReplicateAPI
from .replicate_utils import VideoUtils, AudioUtils, ImageUtils, cleanup_temp_file

# å˜—è©¦è¼‰å…¥ model_configs
try:
    from .model_configs import REPLICATE_MODELS, get_model_config, get_model_names
    HAS_MODEL_CONFIGS = True
except ImportError:
    HAS_MODEL_CONFIGS = False
    def get_model_names():
        return ["lipsync-2-pro"]

# Try to import ComfyUI's folder_paths
try:
    import folder_paths
except ImportError:
    class FolderPaths:
        @staticmethod
        def get_output_directory():
            return os.path.join(os.getcwd(), "output")
    folder_paths = FolderPaths()


class VideoWrapper:
    """å½±ç‰‡åŒ…è£é¡åˆ¥ï¼Œç›¸å®¹æ–¼ Save Video ç¯€é»"""
    
    def __init__(self, video_path):
        self.video_path = video_path
        self._width = None
        self._height = None
        self._fps = None
        self._frame_count = None
        
        # Load video properties
        if os.path.exists(video_path):
            cap = cv2.VideoCapture(video_path)
            if cap.isOpened():
                self._width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                self._height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                self._fps = cap.get(cv2.CAP_PROP_FPS)
                self._frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                cap.release()
    
    def get_dimensions(self):
        """å›å‚³å½±ç‰‡å°ºå¯¸ (width, height)"""
        return (self._width or 1280, self._height or 720)
    
    def get_fps(self):
        """å›å‚³å½±ç‰‡å¹€ç‡"""
        return self._fps or 30.0
    
    def get_frame_count(self):
        """å›å‚³ç¸½å¹€æ•¸"""
        return self._frame_count or 0
    
    def get_path(self):
        """å›å‚³å½±ç‰‡æª”æ¡ˆè·¯å¾‘"""
        return self.video_path
    
    def save_to(self, output_path, **kwargs):
        """å„²å­˜å½±ç‰‡åˆ°æŒ‡å®šè·¯å¾‘ (Save Video ç¯€é»éœ€è¦)"""
        try:
            if os.path.exists(self.video_path):
                os.makedirs(os.path.dirname(output_path), exist_ok=True)
                format_type = kwargs.get('format', None)
                if format_type:
                    print(f"ğŸ“ å„²å­˜å½±ç‰‡æ ¼å¼: {format_type}")
                shutil.copy2(self.video_path, output_path)
                print(f"âœ… å½±ç‰‡å·²å„²å­˜è‡³: {output_path}")
                return output_path
            else:
                print(f"âŒ ä¾†æºå½±ç‰‡ä¸å­˜åœ¨: {self.video_path}")
                return None
        except Exception as e:
            print(f"âŒ å„²å­˜å½±ç‰‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None
        
    def __str__(self):
        return self.video_path


# ======================
# åŸºç¤ Lipsync ç¯€é»
# ======================

class SyncLipsyncNode:
    """
    ComfyUI ç¯€é»ï¼šä½¿ç”¨ Replicate çš„ sync/lipsync-2-pro æ¨¡å‹ç”Ÿæˆå˜´å”‡åŒæ­¥å½±ç‰‡
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "video": ("IMAGE",),
                "audio": ("AUDIO",),
            },
            "optional": {
                "output_filename": ("STRING", {"default": "lipsync_output"}),
                "sync_mode": (["loop", "trim"], {"default": "loop"}),
                "temperature": ("FLOAT", {"default": 0.5, "min": 0.0, "max": 1.0, "step": 0.1}),
                "active_speaker": ("BOOLEAN", {"default": False}),
            },
        }
    
    RETURN_TYPES = ("SYNC_VIDEO", "SYNC_AUDIO")
    RETURN_NAMES = ("video_paths", "audio_paths")
    FUNCTION = "generate_lipsync"
    CATEGORY = "video/lipsync"
    
    def generate_lipsync(self, video, audio, output_filename="lipsync_output", 
                        sync_mode="loop", temperature=0.5, active_speaker=False):
        """ç”Ÿæˆå˜´å”‡åŒæ­¥å½±ç‰‡"""
        print("=" * 60)
        print("ğŸ¬ Replicate Lipsync ç”Ÿæˆ (sync/lipsync-2-pro)")
        print("=" * 60)
        
        temp_files = []
        
        try:
            api = SyncAPI()
            
            print("ğŸï¸ è™•ç†å½±ç‰‡è¼¸å…¥...")
            fps = 24
            final_video_path = VideoUtils.save_image_sequence_to_video(video, fps=fps)
            if final_video_path:
                temp_files.append(final_video_path)
            else:
                print("âŒ è™•ç†å½±ç‰‡è¼¸å…¥å¤±æ•—ï¼")
                return ([], [])
            
            print("ğŸµ è™•ç†éŸ³è¨Šè¼¸å…¥...")
            final_audio_path = AudioUtils.save_audio_from_comfyui(audio)
            if final_audio_path:
                temp_files.append(final_audio_path)
            else:
                print("âŒ è™•ç†éŸ³è¨Šè¼¸å…¥å¤±æ•—ï¼")
                return ([], [])
            
            print("=" * 60)
            print("ğŸ“¤ ä¸Šå‚³æª”æ¡ˆåˆ° Replicate...")
            print("=" * 60)
            
            result_video_path = api.generate_lipsync(
                video_path=final_video_path,
                audio_path=final_audio_path,
                output_filename=output_filename,
                model="lipsync-2-pro",
                sync_mode=sync_mode,
                temperature=temperature,
                active_speaker=active_speaker
            )
            
            for temp_file in temp_files:
                cleanup_temp_file(temp_file)
            
            if result_video_path and os.path.exists(result_video_path):
                print("=" * 60)
                print("âœ… Lipsync ç”Ÿæˆå®Œæˆï¼")
                print(f"ğŸ“ è¼¸å‡º: {result_video_path}")
                print("=" * 60)
                
                video_paths = [result_video_path]
                audio_paths = [final_audio_path] if final_audio_path and os.path.exists(final_audio_path) else []
                
                return (video_paths, audio_paths)
            else:
                print("=" * 60)
                print("âŒ Lipsync ç”Ÿæˆå¤±æ•—ï¼")
                print("=" * 60)
                return ([], [])
                
        except ValueError as e:
            print(f"âŒ API åˆå§‹åŒ–å¤±æ•—: {str(e)}")
            print("\nè«‹ç¢ºä¿ï¼š")
            print("1. åœ¨ .env æª”æ¡ˆä¸­è¨­å®š REPLICATE_API_TOKEN")
            print("2. å¾ä»¥ä¸‹ç¶²å€å–å¾— API token: https://replicate.com/account/api-tokens")
            
            for temp_file in temp_files:
                cleanup_temp_file(temp_file)
            
            return ([], [])
        except Exception as e:
            print(f"âŒ ç”Ÿæˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
            
            for temp_file in temp_files:
                cleanup_temp_file(temp_file)
            
            return ([], [])


class SyncVideoOutput:
    """
    è¼¸å‡ºç¯€é»ï¼šå°‡ Sync lipsync çµæœè½‰æ›ç‚º VIDEO æ ¼å¼
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "video_paths": ("SYNC_VIDEO",),
            },
            "optional": {
                "audio_paths": ("SYNC_AUDIO",),
            }
        }
    
    RETURN_TYPES = ("VIDEO",)
    RETURN_NAMES = ("video",)
    FUNCTION = "output_video"
    CATEGORY = "video/lipsync"
    
    def output_video(self, video_paths, audio_paths=None):
        if not video_paths:
            print("âš ï¸ è­¦å‘Šï¼šæ²’æœ‰ç”Ÿæˆå½±ç‰‡ã€‚")
            return (None,)
        
        video_path = video_paths[0] if isinstance(video_paths, list) else video_paths
        
        if not os.path.exists(video_path):
            print(f"âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ°å½±ç‰‡æª”æ¡ˆ: {video_path}")
            return (None,)
        
        print(f"âœ… å½±ç‰‡æª”æ¡ˆå°±ç·’: {video_path}")
        
        if audio_paths and len(audio_paths) > 0:
            try:
                audio_path = audio_paths[0]
                if os.path.exists(audio_path):
                    print(f"âœ… éŸ³è¨Šæª”æ¡ˆå¯ç”¨: {audio_path}")
                else:
                    print(f"âš ï¸ æ‰¾ä¸åˆ°éŸ³è¨Šæª”æ¡ˆ: {audio_path}")
            except Exception as e:
                print(f"âš ï¸ è™•ç†éŸ³è¨Šå¤±æ•—: {e}")
        
        video_wrapper = VideoWrapper(video_path)
        print(f"âœ… å·²å»ºç«‹å½±ç‰‡åŒ…è£å™¨: {video_wrapper.get_dimensions()}")
        
        return (video_wrapper,)


# ======================
# é€šç”¨æ¨¡å‹ç¯€é»
# ======================

class ReplicateDynamicNode:
    """
    å‹•æ…‹ç¯€é»ï¼šæ ¹æ“šé¸æ“‡çš„æ¨¡å‹åªè™•ç†ç›¸é—œåƒæ•¸
    æ”¯æ´ Soraã€Veo ç­‰æ‰€æœ‰ Replicate æ¨¡å‹
    
    æ³¨æ„ï¼šComfyUI çš„é™åˆ¶ä½¿å¾—ç„¡æ³•çœŸæ­£éš±è—åƒæ•¸ï¼Œ
    ä½†ç¯€é»æœƒè‡ªå‹•å¿½ç•¥ç•¶å‰æ¨¡å‹ä¸éœ€è¦çš„åƒæ•¸ã€‚
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        model_list = get_model_names() if HAS_MODEL_CONFIGS else ["lipsync-2-pro"]
        
        return {
            "required": {
                "model": (model_list, {
                    "default": "sora-2" if "sora-2" in model_list else model_list[0],
                    "tooltip": "é¸æ“‡æ¨¡å‹ / Select Model - ç¯€é»å°‡è‡ªå‹•ä½¿ç”¨è©²æ¨¡å‹éœ€è¦çš„åƒæ•¸ / Node will automatically use parameters required by this model"
                }),
            },
            "optional": {
                # === æ–‡å­—è¼¸å…¥ / Text Inputs ===
                "prompt": ("STRING", {
                    "default": "", 
                    "multiline": True,
                    "tooltip": "æç¤ºè© / Prompt - ç”¨æ–¼ Sora, Veo, MiniMax ç­‰ / Used for Sora, Veo, MiniMax, etc."
                }),
                
                # === åœ–ç‰‡è¼¸å…¥ / Image Inputs ===
                "image": ("IMAGE", {
                    "tooltip": "è¼¸å…¥åœ–ç‰‡ / Input Image - Veo, Wan, SVD éœ€è¦ / Required for Veo, Wan, SVD"
                }),
                "input_reference": ("IMAGE", {
                    "tooltip": "åƒè€ƒåœ–ç‰‡ / Reference Image - Sora 2 ä½¿ç”¨ / Used for Sora 2"
                }),
                "first_frame_image": ("IMAGE", {
                    "tooltip": "é¦–å¹€åœ–ç‰‡ / First Frame - MiniMax ä½¿ç”¨ / Used for MiniMax"
                }),
                "last_frame": ("IMAGE", {
                    "tooltip": "æœ«å¹€åœ–ç‰‡ / Last Frame - Veo ä½¿ç”¨ / Used for Veo"
                }),
                
                # === å½±ç‰‡/éŸ³è¨Šè¼¸å…¥ / Video/Audio Inputs ===
                "video": ("VIDEO", {
                    "tooltip": "è¼¸å…¥å½±ç‰‡ / Input Video - Lipsync ä½¿ç”¨ / Used for Lipsync"
                }),
                "audio": ("AUDIO", {
                    "tooltip": "è¼¸å…¥éŸ³è¨Š / Input Audio - Lipsync, MusicGen ä½¿ç”¨ / Used for Lipsync, MusicGen"
                }),
                
                # === è¼¸å‡ºè¨­å®š / Output Settings ===
                "output_filename": ("STRING", {
                    "default": "replicate_output",
                    "tooltip": "è¼¸å‡ºæª”å / Output Filename"
                }),
                
                # === Sora/Veo åƒæ•¸ / Sora/Veo Parameters ===
                "aspect_ratio": (["portrait", "landscape", "square", "1:1", "16:9", "9:16", "4:3", "3:4", "21:9", "9:21"], {
                    "default": "landscape",
                    "tooltip": "é•·å¯¬æ¯” / Aspect Ratio - Sora, FLUX ä½¿ç”¨ / Used for Sora, FLUX"
                }),
                "resolution": (["480p", "720p", "1080p"], {
                    "default": "720p",
                    "tooltip": "è§£æåº¦ / Resolution - Veo ä½¿ç”¨ / Used for Veo"
                }),
                "quality": (["480p", "720p", "1080p"], {
                    "default": "1080p",
                    "tooltip": "å½±ç‰‡å“è³ª / Video Quality - PixVerse ä½¿ç”¨ / Used for PixVerse"
                }),
                
                # === Lipsync åƒæ•¸ / Lipsync Parameters ===
                "sync_mode": (["loop", "trim"], {
                    "default": "loop",
                    "tooltip": "åŒæ­¥æ¨¡å¼ / Sync Mode - Lipsync ä½¿ç”¨ / Used for Lipsync"
                }),
                "temperature": ("FLOAT", {
                    "default": 0.5, "min": 0.0, "max": 2.0, "step": 0.1,
                    "tooltip": "æº«åº¦ / Temperature - Lipsync, LLM ä½¿ç”¨ / Used for Lipsync, LLM"
                }),
                "active_speaker": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "å•Ÿç”¨ç™¼è¨€è€…åµæ¸¬ / Active Speaker Detection - Lipsync ä½¿ç”¨ / Used for Lipsync"
                }),
                
                # === åœ–ç‰‡ç”Ÿæˆåƒæ•¸ / Image Generation Parameters ===
                "guidance": ("FLOAT", {
                    "default": 3.5, "min": 1.5, "max": 5.0, "step": 0.1,
                    "tooltip": "å¼•å°å¼·åº¦ / Guidance - FLUX ä½¿ç”¨ / Used for FLUX"
                }),
                "guidance_scale": ("FLOAT", {
                    "default": 7.5, "min": 1.0, "max": 20.0, "step": 0.5,
                    "tooltip": "å¼•å°æ¯”ä¾‹ / Guidance Scale - Wan, SVD ä½¿ç”¨ / Used for Wan, SVD"
                }),
                "output_format": (["webp", "jpg", "png"], {
                    "default": "webp",
                    "tooltip": "è¼¸å‡ºæ ¼å¼ / Output Format - åœ–ç‰‡ç”Ÿæˆä½¿ç”¨ / Used for image generation"
                }),
                "output_quality": ("INT", {
                    "default": 80, "min": 0, "max": 100, "step": 1,
                    "tooltip": "è¼¸å‡ºå“è³ª / Output Quality - åœ–ç‰‡ç”Ÿæˆä½¿ç”¨ / Used for image generation"
                }),
                
                # === å½±ç‰‡ç”Ÿæˆåƒæ•¸ / Video Generation Parameters ===
                "num_inference_steps": ("INT", {
                    "default": 50, "min": 1, "max": 100, "step": 1,
                    "tooltip": "æ¨ç†æ­¥æ•¸ / Inference Steps - å½±ç‰‡ç”Ÿæˆä½¿ç”¨ / Used for video generation"
                }),
                "num_frames": ("INT", {
                    "default": 81, "min": 1, "max": 200, "step": 1,
                    "tooltip": "å¹€æ•¸ / Frame Count - Wan, SVD ä½¿ç”¨ / Used for Wan, SVD"
                }),
                "fps": ("INT", {
                    "default": 6, "min": 1, "max": 30, "step": 1,
                    "tooltip": "æ¯ç§’å¹€æ•¸ / FPS - SVD ä½¿ç”¨ / Used for SVD"
                }),
                "duration": ("INT", {
                    "default": 8, "min": 1, "max": 30, "step": 1,
                    "tooltip": "æ™‚é•·(ç§’) / Duration (seconds) - å½±ç‰‡ç”Ÿæˆä½¿ç”¨ / Used for video generation"
                }),
                
                # === é€²éšåƒæ•¸ / Advanced Parameters ===
                "prompt_optimizer": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "æç¤ºè©å„ªåŒ– / Prompt Optimizer - MiniMax ä½¿ç”¨ / Used for MiniMax"
                }),
                "face_enhance": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "è‡‰éƒ¨å¢å¼· / Face Enhancement - Real-ESRGAN ä½¿ç”¨ / Used for Real-ESRGAN"
                }),
                "scale": ("INT", {
                    "default": 2, "min": 1, "max": 4, "step": 1,
                    "tooltip": "æ”¾å¤§å€æ•¸ / Upscale Factor - Real-ESRGAN ä½¿ç”¨ / Used for Real-ESRGAN"
                }),
                "motion_bucket_id": ("INT", {
                    "default": 127, "min": 1, "max": 255, "step": 1,
                    "tooltip": "é‹å‹•å¼·åº¦ / Motion Bucket - SVD ä½¿ç”¨ / Used for SVD"
                }),
            },
        }
    
    RETURN_TYPES = ("REPLICATE_VIDEO", "REPLICATE_AUDIO", "IMAGE", "STRING")
    RETURN_NAMES = ("video_paths", "audio_paths", "image", "info")
    FUNCTION = "run_model"
    CATEGORY = "replicate/dynamic"
    
    def run_model(self, model, prompt="", image=None, input_reference=None, first_frame_image=None,
                 last_frame=None, video=None, audio=None, output_filename="replicate_output", **kwargs):
        """
        åŸ·è¡Œé¸æ“‡çš„ Replicate æ¨¡å‹ / Run selected Replicate model
        è‡ªå‹•éæ¿¾ä¸¦åªä½¿ç”¨è©²æ¨¡å‹éœ€è¦çš„åƒæ•¸ / Automatically filters and uses only required parameters
        """
        config = get_model_config(model) if HAS_MODEL_CONFIGS else None
        if not config:
            print(f"âŒ æœªçŸ¥æ¨¡å‹: {model}")
            return ("", "")
        
        print("=" * 60)
        print(f"ğŸ¤– Replicate: {config['display_name']}")
        print(f"ğŸ“‹ æ¨¡å‹ID / Model ID: {model}")
        print("=" * 60)
        
        temp_files = []
        
        try:
            api = ReplicateAPI()
            
            # å»ºç«‹è¼¸å…¥åƒæ•¸
            inputs = {}
            model_inputs = config.get("inputs", {})
            
            for input_name, input_config in model_inputs.items():
                input_type = input_config.get("type")
                is_required = input_config.get("required", False)
                
                if input_type == "STRING" and input_name == "prompt":
                    if prompt or is_required:
                        inputs[input_name] = prompt
                        
                elif input_type == "IMAGE":
                    image_param = None
                    if input_name == "image" and image is not None:
                        image_param = image
                    elif input_name == "input_reference" and input_reference is not None:
                        image_param = input_reference
                    elif input_name == "first_frame_image" and first_frame_image is not None:
                        image_param = first_frame_image
                    elif input_name == "last_frame" and last_frame is not None:
                        image_param = last_frame
                    
                    if image_param is not None:
                        image_path = ImageUtils.save_image_tensor(image_param)
                        if image_path:
                            temp_files.append(image_path)
                            image_url = api.upload_file(image_path)
                            if image_url:
                                inputs[input_name] = image_url
                    elif is_required:
                        print(f"âš ï¸ å¿…è¦çš„åœ–ç‰‡åƒæ•¸ '{input_name}' æœªæä¾›")
                        
                elif input_type == "VIDEO" and video is not None:
                    video_path = self._extract_path(video)
                    if video_path and os.path.exists(video_path):
                        video_url = api.upload_file(video_path)
                        if video_url:
                            inputs[input_name] = video_url
                            
                elif input_type == "AUDIO" and audio is not None:
                    audio_path = AudioUtils.save_audio_from_comfyui(audio)
                    if audio_path:
                        temp_files.append(audio_path)
                        audio_url = api.upload_file(audio_path)
                        if audio_url:
                            inputs[input_name] = audio_url
                            
                elif input_type in ["COMBO", "FLOAT", "INT", "BOOLEAN"]:
                    if input_name in kwargs and kwargs[input_name] is not None:
                        inputs[input_name] = kwargs[input_name]
                    elif is_required and "default" in input_config:
                        inputs[input_name] = input_config["default"]
            
            print(f"ğŸ“¤ ä¸Šå‚³ä¸¦åŸ·è¡Œæ¨¡å‹ / Uploading and running model...")
            print(f"ğŸ“ ä½¿ç”¨çš„åƒæ•¸ / Used parameters: {list(inputs.keys())}")
            
            # åˆ—å‡ºå¿½ç•¥çš„åƒæ•¸ï¼ˆæä¾›çš„ä½†æ¨¡å‹ä¸éœ€è¦çš„ï¼‰
            all_provided = {k: v for k, v in kwargs.items() if v is not None}
            all_provided.update({'prompt': prompt, 'image': image, 'input_reference': input_reference, 
                               'first_frame_image': first_frame_image, 'last_frame': last_frame,
                               'video': video, 'audio': audio})
            all_provided = {k: v for k, v in all_provided.items() if v is not None and v != "" and v != []}
            ignored = set(all_provided.keys()) - set(inputs.keys()) - {'output_filename'}
            if ignored:
                print(f"â„¹ï¸  å¿½ç•¥çš„åƒæ•¸ / Ignored parameters: {sorted(ignored)}")
            
            result = api.run_model(model, inputs, output_filename)
            
            for temp_file in temp_files:
                cleanup_temp_file(temp_file)
            
            # è™•ç†çµæœ
            video_path = ""
            audio_path = ""
            
            if isinstance(result, list) and len(result) >= 2:
                video_path = result[0] if result[0] else ""
                audio_path = result[1] if result[1] else ""
            elif isinstance(result, str):
                video_path = result
                if config.get("has_audio"):
                    audio_path = result.replace(".mp4", ".wav").replace(".mp4", "_audio.wav")
                    if not os.path.exists(audio_path):
                        audio_path = ""
            
            if video_path and os.path.exists(video_path):
                print("=" * 60)
                print("âœ… ç”Ÿæˆå®Œæˆï¼")
                print(f"ğŸ“ å½±ç‰‡: {video_path}")
                if audio_path:
                    print(f"ğŸµ éŸ³è¨Š: {audio_path}")
                print("=" * 60)
                
                # æå–ç¬¬ä¸€å¹€ä½œç‚ºåœ–ç‰‡
                first_frame = None
                try:
                    import cv2
                    cap = cv2.VideoCapture(video_path)
                    ret, frame = cap.read()
                    cap.release()
                    if ret:
                        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                        first_frame = torch.from_numpy(frame_rgb).float() / 255.0
                        first_frame = first_frame.unsqueeze(0)
                        print(f"ğŸ–¼ï¸ å·²æå–ç¬¬ä¸€å¹€: {first_frame.shape}")
                except Exception as e:
                    print(f"âš ï¸ æå–ç¬¬ä¸€å¹€å¤±æ•—: {e}")
                    first_frame = torch.zeros((1, 512, 512, 3))
                
                if first_frame is None:
                    first_frame = torch.zeros((1, 512, 512, 3))
                
                # å»ºç«‹åŸ·è¡Œè³‡è¨Š
                info_lines = []
                info_lines.append("âœ… åŸ·è¡ŒæˆåŠŸ / Execution Successful")
                info_lines.append(f"ğŸ¤– æ¨¡å‹ / Model: {config['display_name']}")
                info_lines.append(f"ğŸ“ ä½¿ç”¨çš„åƒæ•¸ / Used Parameters: {', '.join(inputs.keys())}")
                if ignored:
                    info_lines.append(f"â„¹ï¸  å¿½ç•¥çš„åƒæ•¸ / Ignored: {', '.join(sorted(ignored))}")
                info_lines.append(f"ğŸ“ å½±ç‰‡è·¯å¾‘ / Video: {video_path}")
                if audio_path:
                    info_lines.append(f"ğŸµ éŸ³è¨Šè·¯å¾‘ / Audio: {audio_path}")
                info_text = "\n".join(info_lines)
                
                # è¿”å›åˆ—è¡¨æ ¼å¼
                video_paths = [video_path]
                audio_paths = [audio_path] if audio_path else []
                return (video_paths, audio_paths, first_frame, info_text)
            else:
                print("=" * 60)
                print("âŒ ç”Ÿæˆå¤±æ•—ï¼")
                print("=" * 60)
                
                # å»ºç«‹å¤±æ•—è³‡è¨Š
                info_text = f"âŒ åŸ·è¡Œå¤±æ•— / Execution Failed\nğŸ¤– æ¨¡å‹ / Model: {config['display_name']}\nâš ï¸ è«‹æª¢æŸ¥åƒæ•¸å’Œ API ç‹€æ…‹"
                return ([], [], torch.zeros((1, 512, 512, 3)), info_text)
                
        except Exception as e:
            print(f"âŒ ç”Ÿæˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
            
            for temp_file in temp_files:
                cleanup_temp_file(temp_file)
            
            # å»ºç«‹éŒ¯èª¤è³‡è¨Š
            error_info = f"âŒ åŸ·è¡ŒéŒ¯èª¤ / Execution Error\nğŸ¤– æ¨¡å‹ / Model: {model}\nâš ï¸ éŒ¯èª¤è¨Šæ¯ / Error: {str(e)}"
            return ([], [], torch.zeros((1, 512, 512, 3)), error_info)
    
    def _extract_path(self, video_input):
        """å¾å½±ç‰‡è¼¸å…¥æå–æª”æ¡ˆè·¯å¾‘"""
        if isinstance(video_input, str):
            return video_input
        elif hasattr(video_input, 'video_path'):
            return video_input.video_path
        elif hasattr(video_input, 'filename'):
            return video_input.filename
        elif isinstance(video_input, dict):
            return video_input.get('video') or video_input.get('filename') or video_input.get('path')
        return None


class ReplicateVideoOutput:
    """
    å°‡ Replicate å½±ç‰‡è¼¸å‡ºè½‰æ›ç‚º VIDEO æ ¼å¼
    ç›¸å®¹æ–¼ Save Video ç¯€é»
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "video_paths": ("REPLICATE_VIDEO",),
            },
            "optional": {
                "audio_paths": ("REPLICATE_AUDIO",),
            }
        }
    
    RETURN_TYPES = ("VIDEO",)
    RETURN_NAMES = ("video",)
    FUNCTION = "output_video"
    CATEGORY = "replicate/output"
    
    def output_video(self, video_paths, audio_paths=None):
        """å°‡å½±ç‰‡è·¯å¾‘è½‰æ›ç‚º VIDEO æ ¼å¼"""
        if not video_paths:
            print("âš ï¸ è­¦å‘Šï¼šæ²’æœ‰ç”Ÿæˆå½±ç‰‡ã€‚")
            return (None,)
        
        # å–å¾—ç¬¬ä¸€å€‹å½±ç‰‡è·¯å¾‘
        video_path = video_paths[0] if isinstance(video_paths, list) else video_paths
        
        if not os.path.exists(video_path):
            print(f"âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ°å½±ç‰‡æª”æ¡ˆ: {video_path}")
            return (None,)
        
        print(f"âœ… å½±ç‰‡æª”æ¡ˆå°±ç·’: {video_path}")
        
        # è™•ç†éŸ³è¨Šè³‡è¨Šï¼ˆå¦‚æœ‰æä¾›ï¼‰
        if audio_paths and len(audio_paths) > 0:
            try:
                audio_path = audio_paths[0]
                if os.path.exists(audio_path):
                    print(f"âœ… éŸ³è¨Šæª”æ¡ˆå¯ç”¨: {audio_path}")
                else:
                    print(f"âš ï¸ æ‰¾ä¸åˆ°éŸ³è¨Šæª”æ¡ˆ: {audio_path}")
            except Exception as e:
                print(f"âš ï¸ è™•ç†éŸ³è¨Šå¤±æ•—: {e}")
        
        # å»ºç«‹ VideoWrapper ç‰©ä»¶ä¾› Save Video ç¯€é»ä½¿ç”¨
        video_wrapper = VideoWrapper(video_path)
        print(f"âœ… å·²å»ºç«‹å½±ç‰‡åŒ…è£å™¨: {video_wrapper.get_dimensions()}")
        
        return (video_wrapper,)


class ReplicateAudioOutput:
    """
    è¼¸å‡º Replicate ç”Ÿæˆçš„éŸ³è¨Šæª”æ¡ˆ
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "audio_paths": ("REPLICATE_AUDIO",),
            },
        }
    
    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("audio",)
    FUNCTION = "output_audio"
    CATEGORY = "replicate/output"
    
    def output_audio(self, audio_paths):
        """è¼‰å…¥ä¸¦è¼¸å‡ºéŸ³è¨Šæª”æ¡ˆ"""
        if not audio_paths:
            print("âš ï¸ è­¦å‘Šï¼šæ²’æœ‰ç”ŸæˆéŸ³è¨Šã€‚")
            return (None,)
        
        # å–å¾—ç¬¬ä¸€å€‹éŸ³è¨Šè·¯å¾‘
        audio_path = audio_paths[0] if isinstance(audio_paths, list) else audio_paths
        
        if not os.path.exists(audio_path):
            print(f"âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ°éŸ³è¨Šæª”æ¡ˆ: {audio_path}")
            return (None,)
        
        try:
            import soundfile as sf
            waveform, sample_rate = sf.read(audio_path)
            
            waveform_tensor = torch.from_numpy(waveform).float()
            
            if len(waveform_tensor.shape) == 1:
                waveform_tensor = waveform_tensor.unsqueeze(0)
            elif len(waveform_tensor.shape) == 2:
                if waveform_tensor.shape[0] > waveform_tensor.shape[1]:
                    waveform_tensor = waveform_tensor.transpose(0, 1)
            
            audio_dict = {
                "waveform": waveform_tensor,
                "sample_rate": sample_rate
            }
            
            print(f"âœ… éŸ³è¨Šå·²è¼‰å…¥: {audio_path}")
            print(f"   å–æ¨£ç‡: {sample_rate}Hz")
            print(f"   å½¢ç‹€: {waveform_tensor.shape}")
            
            return (audio_dict,)
            
        except Exception as e:
            print(f"âŒ è¼‰å…¥éŸ³è¨Šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
            return (None,)


class ReplicateVideoAudioMerge:
    """
    åˆä½µå½±ç‰‡å’ŒéŸ³è¨Šç‚ºå–®ä¸€å½±ç‰‡æª”æ¡ˆ
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "video_paths": ("REPLICATE_VIDEO",),
                "audio_paths": ("REPLICATE_AUDIO",),
            },
            "optional": {
                "output_filename": ("STRING", {"default": "merged_video"}),
            }
        }
    
    RETURN_TYPES = ("VIDEO",)
    RETURN_NAMES = ("video",)
    FUNCTION = "merge_video_audio"
    CATEGORY = "replicate/output"
    
    def merge_video_audio(self, video_paths, audio_paths, output_filename="merged_video"):
        """ä½¿ç”¨ ffmpeg åˆä½µå½±ç‰‡å’ŒéŸ³è¨Š"""
        if not video_paths:
            print("âŒ æ²’æœ‰æä¾›å½±ç‰‡")
            return (None,)
        
        # å–å¾—ç¬¬ä¸€å€‹å½±ç‰‡å’ŒéŸ³è¨Šè·¯å¾‘
        video_path = video_paths[0] if isinstance(video_paths, list) else video_paths
        
        if not os.path.exists(video_path):
            print(f"âŒ æ‰¾ä¸åˆ°å½±ç‰‡æª”æ¡ˆ: {video_path}")
            return (None,)
        
        if not audio_paths or len(audio_paths) == 0:
            print("âš ï¸ æ²’æœ‰æä¾›éŸ³è¨Šï¼Œå›å‚³åŸå§‹å½±ç‰‡")
            return (VideoWrapper(video_path),)
        
        audio_path = audio_paths[0] if isinstance(audio_paths, list) else audio_paths
        
        if not os.path.exists(audio_path):
            print(f"âš ï¸ æ‰¾ä¸åˆ°éŸ³è¨Šæª”æ¡ˆ: {audio_path}ï¼Œå›å‚³åŸå§‹å½±ç‰‡")
            return (VideoWrapper(video_path),)
        
        try:
            import subprocess
            
            output_dir = folder_paths.get_output_directory()
            os.makedirs(output_dir, exist_ok=True)
            
            import time
            timestamp = int(time.time())
            output_path = os.path.join(output_dir, f"{output_filename}_{timestamp}.mp4")
            
            cmd = [
                'ffmpeg',
                '-i', video_path,
                '-i', audio_path,
                '-c:v', 'copy',
                '-c:a', 'aac',
                '-strict', 'experimental',
                '-shortest',
                '-y',
                output_path
            ]
            
            print(f"ğŸ”„ åˆä½µå½±ç‰‡å’ŒéŸ³è¨Š...")
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0 and os.path.exists(output_path):
                print(f"âœ… å·²å„²å­˜åˆä½µå½±ç‰‡: {output_path}")
                return (VideoWrapper(output_path),)
            else:
                print(f"âŒ FFmpeg åˆä½µå¤±æ•—: {result.stderr}")
                print("âš ï¸ å›å‚³åŸå§‹å½±ç‰‡")
                return (VideoWrapper(video_path),)
                
        except Exception as e:
            print(f"âŒ åˆä½µå½±ç‰‡å’ŒéŸ³è¨Šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            print("âš ï¸ å›å‚³åŸå§‹å½±ç‰‡")
            return (VideoWrapper(video_path),)


# ======================
# ç°¡åŒ–ç¯€é»ï¼ˆå¯é¸ï¼‰
# ======================

class ReplicateTextToVideoNode:
    """æ–‡å­—ç”Ÿæˆå½±ç‰‡"""
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "prompt": ("STRING", {"default": "", "multiline": True}),
                "model": (["minimax-video-01", "minimax-video-01-live"], {"default": "minimax-video-01"}),
            },
            "optional": {
                "first_frame_image": ("IMAGE",),
                "prompt_optimizer": ("BOOLEAN", {"default": True}),
                "output_filename": ("STRING", {"default": "text_to_video"}),
            },
        }
    
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("video_path",)
    FUNCTION = "generate_video"
    CATEGORY = "replicate/video"
    
    def generate_video(self, prompt, model="minimax-video-01", first_frame_image=None, 
                      prompt_optimizer=True, output_filename="text_to_video"):
        """å¾æ–‡å­—ç”Ÿæˆå½±ç‰‡"""
        print("=" * 60)
        print(f"ğŸ¬ æ–‡å­—ç”Ÿæˆå½±ç‰‡: {model}")
        print("=" * 60)
        
        temp_files = []
        
        try:
            api = ReplicateAPI()
            
            inputs = {
                "prompt": prompt,
                "prompt_optimizer": prompt_optimizer,
            }
            
            if first_frame_image is not None:
                image_path = ImageUtils.save_image_tensor(first_frame_image)
                if image_path:
                    temp_files.append(image_path)
                    image_url = api.upload_file(image_path)
                    if image_url:
                        inputs["first_frame_image"] = image_url
            
            result_path = api.run_model(model, inputs, output_filename)
            
            for temp_file in temp_files:
                cleanup_temp_file(temp_file)
            
            if result_path:
                print(f"âœ… å½±ç‰‡å·²ç”Ÿæˆ: {result_path}")
                return (result_path,)
            else:
                return ("",)
                
        except Exception as e:
            print(f"âŒ éŒ¯èª¤: {e}")
            for temp_file in temp_files:
                cleanup_temp_file(temp_file)
            return ("",)


class ReplicateImageToVideoNode:
    """åœ–ç‰‡ç”Ÿæˆå½±ç‰‡"""
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image": ("IMAGE",),
                "model": (["wan", "stable-video-diffusion"], {"default": "wan"}),
            },
            "optional": {
                "prompt": ("STRING", {"default": "", "multiline": True}),
                "num_frames": ("INT", {"default": 81, "min": 1, "max": 200, "step": 1}),
                "guidance_scale": ("FLOAT", {"default": 7.5, "min": 1.0, "max": 20.0, "step": 0.5}),
                "num_inference_steps": ("INT", {"default": 50, "min": 10, "max": 100, "step": 1}),
                "motion_bucket_id": ("INT", {"default": 127, "min": 1, "max": 255, "step": 1}),
                "fps": ("INT", {"default": 6, "min": 1, "max": 30, "step": 1}),
                "output_filename": ("STRING", {"default": "image_to_video"}),
            },
        }
    
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("video_path",)
    FUNCTION = "generate_video"
    CATEGORY = "replicate/video"
    
    def generate_video(self, image, model="wan", prompt="", num_frames=81, 
                      guidance_scale=7.5, num_inference_steps=50, motion_bucket_id=127,
                      fps=6, output_filename="image_to_video"):
        """å¾åœ–ç‰‡ç”Ÿæˆå½±ç‰‡"""
        print("=" * 60)
        print(f"ğŸ¬ åœ–ç‰‡ç”Ÿæˆå½±ç‰‡: {model}")
        print("=" * 60)
        
        temp_files = []
        
        try:
            api = ReplicateAPI()
            
            image_path = ImageUtils.save_image_tensor(image)
            if not image_path:
                print("âŒ å„²å­˜åœ–ç‰‡å¤±æ•—")
                return ("",)
            
            temp_files.append(image_path)
            image_url = api.upload_file(image_path)
            if not image_url:
                print("âŒ ä¸Šå‚³åœ–ç‰‡å¤±æ•—")
                return ("",)
            
            inputs = {"image": image_url}
            
            if model == "wan":
                inputs.update({
                    "prompt": prompt,
                    "num_frames": num_frames,
                    "guidance_scale": guidance_scale,
                    "num_inference_steps": num_inference_steps,
                })
            elif model == "stable-video-diffusion":
                inputs.update({
                    "motion_bucket_id": motion_bucket_id,
                    "fps": fps,
                })
            
            result_path = api.run_model(model, inputs, output_filename)
            
            for temp_file in temp_files:
                cleanup_temp_file(temp_file)
            
            if result_path:
                print(f"âœ… å½±ç‰‡å·²ç”Ÿæˆ: {result_path}")
                return (result_path,)
            else:
                return ("",)
                
        except Exception as e:
            print(f"âŒ éŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
            for temp_file in temp_files:
                cleanup_temp_file(temp_file)
            return ("",)


class ReplicateImageGenNode:
    """åœ–ç‰‡ç”Ÿæˆ"""
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "prompt": ("STRING", {"default": "", "multiline": True}),
                "model": (["flux-schnell", "flux-dev", "luma-photon", "luma-photon-flash"], 
                         {"default": "flux-schnell"}),
            },
            "optional": {
                "aspect_ratio": (["1:1", "16:9", "9:16", "4:3", "3:4", "21:9", "9:21"], 
                               {"default": "1:1"}),
                "guidance": ("FLOAT", {"default": 3.5, "min": 1.5, "max": 5.0, "step": 0.1}),
                "num_inference_steps": ("INT", {"default": 28, "min": 1, "max": 50, "step": 1}),
                "output_format": (["webp", "jpg", "png"], {"default": "webp"}),
                "output_quality": ("INT", {"default": 80, "min": 0, "max": 100, "step": 1}),
                "output_filename": ("STRING", {"default": "generated_image"}),
            },
        }
    
    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("image",)
    FUNCTION = "generate_image"
    CATEGORY = "replicate/image"
    
    def generate_image(self, prompt, model="flux-schnell", aspect_ratio="1:1",
                      guidance=3.5, num_inference_steps=28, output_format="webp",
                      output_quality=80, output_filename="generated_image"):
        """å¾æ–‡å­—ç”Ÿæˆåœ–ç‰‡"""
        print("=" * 60)
        print(f"ğŸ–¼ï¸ åœ–ç‰‡ç”Ÿæˆ: {model}")
        print("=" * 60)
        
        try:
            api = ReplicateAPI()
            
            inputs = {
                "prompt": prompt,
                "aspect_ratio": aspect_ratio,
                "output_format": output_format,
                "output_quality": output_quality,
            }
            
            if model == "flux-dev":
                inputs.update({
                    "guidance": guidance,
                    "num_inference_steps": num_inference_steps,
                })
            
            result_path = api.run_model(model, inputs, output_filename)
            
            if result_path and os.path.exists(result_path):
                image = cv2.imread(result_path)
                if image is not None:
                    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                    image_tensor = torch.from_numpy(image_rgb).float() / 255.0
                    image_tensor = image_tensor.unsqueeze(0)
                    print(f"âœ… åœ–ç‰‡å·²è¼‰å…¥: {image_tensor.shape}")
                    return (image_tensor,)
            
            print("âŒ ç”Ÿæˆåœ–ç‰‡å¤±æ•—")
            return (torch.zeros((1, 512, 512, 3)),)
                
        except Exception as e:
            print(f"âŒ éŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
            return (torch.zeros((1, 512, 512, 3)),)


class ReplicateModelInfo:
    """
    é¡¯ç¤ºåŸ·è¡Œè³‡è¨Šç¯€é»
    Shows execution information
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "execution_info": ("STRING", {
                    "default": "",
                    "forceInput": True,
                    "multiline": True,
                }),
            },
        }
    
    RETURN_TYPES = ()
    FUNCTION = "display_info"
    CATEGORY = "replicate/info"
    OUTPUT_NODE = True
    
    def display_info(self, execution_info=""):
        """é¡¯ç¤ºåŸ·è¡Œè³‡è¨Š"""
        if execution_info and execution_info.strip():
            print("\n" + "=" * 70)
            print("ğŸ“Š åŸ·è¡Œè³‡è¨Š / Execution Information")
            print("=" * 70)
            print(execution_info)
            print("=" * 70 + "\n")
        else:
            print("â„¹ï¸  ç­‰å¾…åŸ·è¡Œè³‡è¨Š... / Waiting for execution info...")
        
        return {}


# ======================
# ç¯€é»è¨»å†Š
# ======================

NODE_CLASS_MAPPINGS = {
    # Lipsync ç¯€é»
    "SyncLipsyncNode": SyncLipsyncNode,
    "SyncVideoOutput": SyncVideoOutput,
    
    # å‹•æ…‹ç¯€é»
    "ReplicateDynamicNode": ReplicateDynamicNode,
    
    # è³‡è¨Šç¯€é»
    "ReplicateModelInfo": ReplicateModelInfo,
    
    # è¼¸å‡ºç¯€é»
    "ReplicateVideoOutput": ReplicateVideoOutput,
    "ReplicateAudioOutput": ReplicateAudioOutput,
    "ReplicateVideoAudioMerge": ReplicateVideoAudioMerge,
    
    # å°ˆé–€åŒ–ç¯€é»
    "ReplicateTextToVideoNode": ReplicateTextToVideoNode,
    "ReplicateImageToVideoNode": ReplicateImageToVideoNode,
    "ReplicateImageGenNode": ReplicateImageGenNode,
}

# é›™èªé¡¯ç¤ºåç¨±æ˜ å°„ / Bilingual Display Name Mappings
NODE_DISPLAY_NAME_MAPPINGS = {
    # Lipsync ç¯€é» / Lipsync Nodes
    "SyncLipsyncNode": "ğŸ­ Sync Lipsync ç”Ÿæˆ / Generate",
    "SyncVideoOutput": "ğŸ“¹ Sync å½±ç‰‡è¼¸å‡º / Video Output",
    
    # å‹•æ…‹ç¯€é» / Dynamic Node (ä¸»è¦ç¯€é» Main Node)
    "ReplicateDynamicNode": "ğŸ¬ Replicate å‹•æ…‹ / Dynamic (All Models)",
    
    # è³‡è¨Šç¯€é» / Info Node
    "ReplicateModelInfo": "ğŸ“‹ æ¨¡å‹è³‡è¨Š / Model Info",
    
    # è¼¸å‡ºç¯€é» / Output Nodes
    "ReplicateVideoOutput": "ğŸ“¹ Replicate å½±ç‰‡ / Video Output",
    "ReplicateAudioOutput": "ğŸµ Replicate éŸ³è¨Š / Audio Output",
    "ReplicateVideoAudioMerge": "ğŸ”„ åˆä½µå½±éŸ³ / Merge Video+Audio",
    
    # å°ˆé–€åŒ–ç¯€é» / Specialized Nodes
    "ReplicateTextToVideoNode": "ğŸ¬ æ–‡å­—è½‰å½±ç‰‡ / Text to Video",
    "ReplicateImageToVideoNode": "ğŸ–¼ï¸ åœ–ç‰‡è½‰å½±ç‰‡ / Image to Video",
    "ReplicateImageGenNode": "ğŸ¨ åœ–ç‰‡ç”Ÿæˆ / Image Generation",
}
